{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "outputs": [],
   "source": [
    "<a href=\"https://colab.research.google.com/github/eaedk/Machine-Learning-Tutorials/blob/main/ML_Step_By_Step_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wW3v1-2n3CzK"
   },
   "outputs": [],
   "source": [
    "# Intro\n",
    "## General\n",
    "Machine learning allows the user to feed a computer algorithm an immense amount of data and have the computer analyze and make data-driven recommendations and decisions based on only the input data. \n",
    "In most of the situations we want to have a machine learning system to make **predictions**, so we have several categories of machine learning tasks depending on the type of prediction needed: **Classification, Regression, Clustering, Generation**, etc.\n",
    "\n",
    "**Classification** is the task whose goal is the prediction of the label of the class to which the input belongs (e.g., Classification of images in two classes: cats and dogs).\n",
    "**Regression** is the task whose goal is the prediction of numerical value(s) related to the input (e.g., House rent prediction, Estimated time of arrival ).\n",
    "**Generation** is the task whose goal is the creation of something new related to the input (e.g., Text translation, Audio beat generation, Image denoising ). **Clustering** is the task of grouping a set of objects in such a way that objects in the same group (called a **cluster**) are more similar (in some sense) to each other than to those in other **clusters** (e.g., Clients clutering).\n",
    "\n",
    "In machine learning, there are learning paradigms that relate to one aspect of the dataset: **the presence of the label to be predicted**. **Supervised Learning** is the paradigm of learning that is applied when the dataset has the label variables to be predicted, known as ` y variables`. **Unsupervised Learning** is the paradigm of learning that is applied when the dataset has not the label variables to be predicted. **Self-supervised Learning** is the paradigm of learning that is applied when part of the X dataset is considere as the label to be predicted (e.g., the Dataset is made of texts and the model try to predict the next word of each sentence).\n",
    "\n",
    "## Notebook overview\n",
    "\n",
    "This notebook is a guide to start practicing Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4VFUnkuexCE"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdFpRxPje1gw"
   },
   "source": [
    "## Installation\n",
    "Here is the section to install all the packages/libraries that will be needed to tackle the challlenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-d-roFQe6yi"
   },
   "outputs": [],
   "source": [
    "#%pip install pyodbc\n",
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenvNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HmLxlQre7HW"
   },
   "source": [
    "## Importation\n",
    "Here is the section to import all the packages/libraries that will be used through this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MP62JaiKfCnS"
   },
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Vizualisation (Matplotlib, Plotly, Seaborn, etc. )\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# EDA (pandas-profiling, hypothesis testing etc. )\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "# Feature Processing (Scikit-learn processing, etc. )\n",
    "...\n",
    "\n",
    "# Machine Learning (Scikit-learn Estimators, Catboost, LightGBM, etc. )\n",
    "...\n",
    "\n",
    "# Hyperparameters Fine-tuning (Scikit-learn hp search, cross-validation, etc. )\n",
    "...\n",
    "\n",
    "# Other packages\n",
    "import os, pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data from database\n",
    "server = \"dap-projects-database.database.windows.net\"\n",
    "database = \"dapDB\"\n",
    "username = \"dataAnalyst_LP2\"\n",
    "password = \"A3g@3kR$2y\"\n",
    "\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "\n",
    "# Connecting to the DB\n",
    "connection = pyodbc.connect(connection_string)\n",
    "\n",
    "# Selecting the Table\n",
    "query = \"Select * from dbo.LP2_Telco_churn_first_3000\"\n",
    "data = pd.read_sql(query, connection)\n",
    "\n",
    "# Preview the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfOADQf0e9i1"
   },
   "source": [
    "# Data Loading\n",
    "Here is the section to load the datasets (train, eval, test) and the additional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIQyG5EcfQlU"
   },
   "outputs": [],
   "source": [
    "# Data loading\n",
    "training_file = \"C:/Users/hp/Documents/GitHub/Customer-Churn-ML-Prediction/Datasets/training_data.csv\"\n",
    "training_data = pd.read_csv(training_file)\n",
    "\n",
    "#Preview Training Data\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okaZxnc3fRId"
   },
   "source": [
    "# Exploratory Data Analysis: EDA\n",
    "Here is the section to **inspect** the datasets in depth, **present** it, make **hypotheses** and **think** the *cleaning, processing and features creation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VNR9LfZfbGe"
   },
   "source": [
    "## Univariate Analysis\n",
    "Univariate analysis focuses on examining individual variables in isolation. For customer churn data, univariate analysis will involve exploring Churn Distribution, Customer Demographics and Service Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn distribution\n",
    "# Calculate the churn rate (percentage of customers who churned)\n",
    "churn_counts = training_data['Churn'].value_counts()\n",
    "churn_percentages = churn_counts / churn_counts.sum() * 100\n",
    "print(\"Churn Distribution:\")\n",
    "print(churn_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The churn distribution shows that approximately 26.5% of customers have churned, while around 73.5% have not churned. This indicates a class imbalance, with the churned class being the minority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer demographics - Gender\n",
    "# Analyzing the gender distributions\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='gender', data=training_data)\n",
    "plt.title('Gender Distribution')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gender distribution analysis reveals the count of customers by gender. It shows that the gender is balanced with male slightly higher than female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer demographics - Senior Citizens\n",
    "# Analyzing the SeniorCitizens distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "training_data['SeniorCitizen'].value_counts().plot(kind='bar')\n",
    "plt.title('Senior Citizens Distribution')\n",
    "plt.xlabel('Senior Citizens')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gender distribution analysis reveals the count of customers by senior citizens. It shows that there are whole lot more of non-senior citizens than senior citizens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service usage - Monthly Charges\n",
    "# Examining the Average monthly charges with a box plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='MonthlyCharges', data=training_data)\n",
    "plt.title('Monthly Charges')\n",
    "plt.xlabel('Charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plot for monthly charges gives an overview of the distribution of charges among customers. It shows the range, median, quartiles, and any potential outliers in the monthly charges. With this, we can see that the average is around 70 with 20 being the lowest and 120 the highest charge per month. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwNIKi8K-WlT"
   },
   "source": [
    "## Bivariate & Multivariate Analysis\n",
    "Explore, analyze, visualize each variable in relation to the others. In the case of customer churn data, bivariate analysis can help uncover potential correlations or dependencies between different variables and churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn by gender\n",
    "# Analyzing the Churn Rate by Gender to observe any patterns associated with churn\n",
    "gender_churn = training_data.groupby('gender')['Churn'].value_counts(normalize=True).unstack().reset_index()\n",
    "gender_churn.rename(columns={'No': 'No Churn', 'Yes': 'Churn'}, inplace=True)\n",
    "\n",
    "fig = px.bar(gender_churn, x='gender', y=['No Churn', 'Churn'], barmode='stack', title='Churn by Gender')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacked bar chart demonstrates the churn rates categorized by gender. It shows that there is about 80% No Churn rate in both Male and Female and about 20% Churn rate in both too, so there's no pattern in genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn by Payment Method\n",
    "# Analyzing the Churn Rate by Payment Method to observe any patterns associated with churn\n",
    "payment_churn = training_data.groupby('PaymentMethod')['Churn'].value_counts(normalize=True).unstack().reset_index()\n",
    "payment_churn.rename(columns={'No': 'No Churn', 'Yes': 'Churn'}, inplace=True)\n",
    "\n",
    "fig = px.bar(payment_churn, x='PaymentMethod', y=['No Churn', 'Churn'], barmode='stack', title='Churn by Payment Method')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacked bar chart demonstrates the churn rates categorized by payment method. It shows that Electronic Checks are definitely not the way to go and should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn and service usage - Monthly Charges\n",
    "# Analyzing the Churn Rate by Monthly Charges to observe any patterns associated with churn\n",
    "fig = px.scatter(training_data, x='MonthlyCharges', y='Churn', color='Churn', title='Churn and Monthly Charges')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot visualizes the relationship between churn and monthly charges. It allows us to observe whether there is any noticeable pattern or trend between higher charges and churn. It can help identify if customers with higher monthly charges are more likely to churn. This shows that charges between 70 to 110 have a higher chance to churn and also not churn which means no noticeable patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn and contract information - Contract\n",
    "# Analyzing the Churn Rate by Contract to observe any patterns associated with churn\n",
    "contract_churn = training_data.groupby('Contract')['Churn'].value_counts(normalize=True).unstack().reset_index()\n",
    "contract_churn.rename(columns={'No': 'No Churn', 'Yes': 'Churn'}, inplace=True)\n",
    "\n",
    "fig = px.bar(contract_churn, x='Contract', y=['No Churn', 'Churn'], barmode='stack', title='Churn by Contract Type')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacked bar chart showcases the churn rates based on different contract types (Month-to-month, One year and Two years). It shows that a signicantly higher percentage of customers are likely to churn with a contract type of Month-to-month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn and customer tenure\n",
    "# Analyzing the Churn Rate by customer tenure to observe any patterns associated with churn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(training_data, x='tenure', hue='Churn', multiple='stack', bins=20, palette='Set1')\n",
    "plt.title('Churn and Customer Tenure')\n",
    "plt.xlabel('Tenure (Months)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram displays the distribution of customer tenure for both churned and non-churned customers over their tenure in months. This shows that customer loyalty goes a very long with the customers with the highest months(70) having the highest No Churn count compared to its churned count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8WB7j-3fxzL"
   },
   "source": [
    "# Feature Processing & Engineering\n",
    "Here is the section to **clean**, **process** the dataset and **create new features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6Rh5SimmcGe"
   },
   "source": [
    "## Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIBrDy1Pmo2_"
   },
   "outputs": [],
   "source": [
    "# Use pandas.DataFrame.drop_duplicates method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxYGw6zug8lI"
   },
   "source": [
    "## Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbsbMbs1hDzo"
   },
   "outputs": [],
   "source": [
    "# Use train_test_split with a random_state, and add stratify for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLJuYG_JgBH8"
   },
   "source": [
    "## Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52POL1F8gLEa"
   },
   "outputs": [],
   "source": [
    "# Use sklearn.impute.SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOoXNG6lgLVD"
   },
   "source": [
    "## New Features Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fU5RQ0itegd4"
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYkNdwQ-gRod"
   },
   "source": [
    "## Features Encoding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99vphpwPgbUw"
   },
   "outputs": [],
   "source": [
    "# From sklearn.preprocessing use OneHotEncoder to encode the categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0TvhRVghEcP"
   },
   "source": [
    "## Features Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pf6WtuHphLBE"
   },
   "outputs": [],
   "source": [
    "# From sklearn.preprocessing use StandardScaler, MinMaxScaler, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-UlAVv4iYz8"
   },
   "source": [
    "## Optional: Train set Balancing (for Classification only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySLn254IiujE"
   },
   "outputs": [],
   "source": [
    "# Use Over-sampling/Under-sampling methods, more details here: https://imbalanced-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WloQgMqqf6bT"
   },
   "source": [
    "# Machine Learning Modeling \n",
    "Here is the section to **build**, **train**, **evaluate** and **compare** the models to each others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTiiqCnG1FA2"
   },
   "source": [
    "## Simple Model #001\n",
    "\n",
    "Please, keep the following structure to try all the model you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46li5Z2z1ME7"
   },
   "source": [
    "### Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Fjv6pJt1uoV"
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwHqTKk11U5f"
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgsct6LH1v9e"
   },
   "outputs": [],
   "source": [
    "# Use the .fit method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6aXGMEf1Yra"
   },
   "source": [
    "### Evaluate the Model on the Evaluation dataset (Evalset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XBrsspR1wso"
   },
   "outputs": [],
   "source": [
    "# Compute the valid metrics for the use case # Optional: show the classification report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "001NdcP11dMu"
   },
   "source": [
    "### Predict on a unknown dataset (Testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWnGeGn0i3b0"
   },
   "outputs": [],
   "source": [
    "# Use .predict method # .predict_proba is available just for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A54gPujOmqaV"
   },
   "source": [
    "## Simple Model #002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MC6WaxZUmqaW"
   },
   "source": [
    "### Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THTw7OjhmqaW"
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7CV9W0DmqaW"
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQMdz0JPmqaW"
   },
   "outputs": [],
   "source": [
    "# Use the .fit method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1Fb-2_PmqaW"
   },
   "source": [
    "### Evaluate the Model on the Evaluation dataset (Evalset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oz3-pxgDmqaW"
   },
   "outputs": [],
   "source": [
    "# Compute the valid metrics for the use case # Optional: show the classification report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-x7GFqxmqaW"
   },
   "source": [
    "### Predict on a unknown dataset (Testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORaQB1zLmqaW"
   },
   "outputs": [],
   "source": [
    "# Use .predict method # .predict_proba is available just for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S73ZUqkUmyz_"
   },
   "source": [
    "## Models comparison\n",
    "Create a pandas dataframe that will allow you to compare your models.\n",
    "\n",
    "Find a sample frame below :\n",
    "\n",
    "|     | Model_Name     | Metric (metric_name)    | Details  |\n",
    "|:---:|:--------------:|:--------------:|:-----------------:|\n",
    "| 0   |  -             |  -             | -                 |\n",
    "| 1   |  -             |  -             | -                 |\n",
    "\n",
    "\n",
    "You might use the pandas dataframe method `.sort_values()` to sort the dataframe regarding the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpwLiOo9pVIQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezmrABneph31"
   },
   "source": [
    "## Hyperparameters tuning \n",
    "\n",
    "Fine-tune the Top-k models (3 < k < 5) using a ` GridSearchCV`  (that is in sklearn.model_selection\n",
    ") to find the best hyperparameters and achieve the maximum performance of each of the Top-k models, then compare them again to select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpTEDK9RrlRl"
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_7_gsrX2VSk"
   },
   "source": [
    "# Export key components\n",
    "Here is the section to **export** the important ML objects that will be use to develop an app: *Encoder, Scaler, ColumnTransformer, Model, Pipeline, etc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32mviWyGAMqP"
   },
   "outputs": [],
   "source": [
    "# Use pickle : put all your key components in a python dictionary and save it as a file that will be loaded in an app"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbGyfKWqwXNuXNuhQpgnBh",
   "collapsed_sections": [
    "wxYGw6zug8lI",
    "S73ZUqkUmyz_"
   ],
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
